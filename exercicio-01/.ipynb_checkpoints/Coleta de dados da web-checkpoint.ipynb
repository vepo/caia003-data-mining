{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coleta de dados da Web\n",
    "\n",
    "Demonstração de alguns recursos importantes para essa tarefa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Victor Emanuel Perticarrari Osório_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carregando Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: TwitterAPI in c:\\python310\\lib\\site-packages (2.7.13)\n",
      "Requirement already satisfied: requests in c:\\python310\\lib\\site-packages (from TwitterAPI) (2.28.1)\n",
      "Requirement already satisfied: requests_oauthlib in c:\\python310\\lib\\site-packages (from TwitterAPI) (1.3.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python310\\lib\\site-packages (from requests->TwitterAPI) (2022.6.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python310\\lib\\site-packages (from requests->TwitterAPI) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\python310\\lib\\site-packages (from requests->TwitterAPI) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\python310\\lib\\site-packages (from requests->TwitterAPI) (2.1.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\python310\\lib\\site-packages (from requests_oauthlib->TwitterAPI) (3.2.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: simplejson in c:\\python310\\lib\\site-packages (3.17.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\python310\\lib\\site-packages (1.4.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\victoros\\appdata\\roaming\\python\\python310\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\python310\\lib\\site-packages (from pandas) (2022.2.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\python310\\lib\\site-packages (from pandas) (1.23.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\victoros\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymongo in c:\\python310\\lib\\site-packages (4.2.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\victoros\\appdata\\roaming\\python\\python310\\site-packages (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\victoros\\appdata\\roaming\\python\\python310\\site-packages (from beautifulsoup4) (2.3.2.post1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "#Instalação das apis - caso não tenha instalado\n",
    "!pip install TwitterAPI\n",
    "!pip install simplejson\n",
    "!pip install pandas\n",
    "!pip install pymongo\n",
    "!pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Item 1\n",
    "\n",
    "Faça uma adaptação no coletor de tweets para coletar qualquer tipo de tweet compartilhado no\n",
    "Brasil. Dica: veja a documentação da biblioteca TwitterApi. Faça uma coleta por alguns minutos,\n",
    "salvando o resultado em um arquivo ou em um banco de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TwitterAPI.TwitterAPI.TwitterAPI object at 0x00000237B7E039A0>\n"
     ]
    }
   ],
   "source": [
    "from TwitterAPI import TwitterAPI\n",
    "\n",
    "\n",
    "## Substitua os valores abaixo\n",
    "## Crie chaves em https://developer.twitter.com/en/portal/apps\n",
    "twitter_api = TwitterAPI(consumer_key='Y7Z8zEPXmtONfn9GnpSRB9muO',\n",
    "                         consumer_secret='KXAI74hdbgqHnACBOIY1OnZmMbpStf23amCeTZ8J6WpT9VHkXr',\n",
    "                         access_token_key='14613563-F3Im28Mmnfvr7BEPjAQ82dwX2otH0CebjKA3VfIFK',\n",
    "                         access_token_secret='NINLLuZMsTiiLpdrEyB7KtAHVjyECq7CxIkF5wGS2PLNs')\n",
    "print(twitter_api)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os dados serão armazenados no MongoDB como foram recebidos.\n",
    "\n",
    "Instale um MongoDB sem restrição de acesso ou execute o container abaixo:\n",
    "\n",
    "```bash\n",
    "docker run --name tweets -p 27017:27017 -d mongo:4\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets coletados: 2801\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "## Tutorial pandas: https://pandas.pydata.org/docs/getting_started/intro_tutorials/01_table_oriented.html#min-tut-01-tableoriented\n",
    "import pandas as pd\n",
    "\n",
    "from pymongo import MongoClient\n",
    "import pymongo\n",
    "\n",
    "## Coordenadas encontradas em https://gist.github.com/graydon/11198540\n",
    "filters = {\"locations\": \"-73.9872354804, -33.7683777809, -34.7299934555, 5.2444863956\"}\n",
    "\n",
    "tweet_stream = twitter_api.request('statuses/filter', filters).get_iterator()\n",
    "TEMPO_TOTAL_EM_MIN = 5\n",
    "CONNECTION_STRING = \"mongodb://localhost:27017/tweets\"\n",
    "client = MongoClient(CONNECTION_STRING)\n",
    "database = client['dados']\n",
    "tweets_collections = database[\"tweets\"]\n",
    "df = None\n",
    "t_inicial = time.time() ## Inicio em segundos\n",
    "for tweet in tweet_stream:\n",
    "    try:\n",
    "        ## Usando id do tweet para evitar tweets repetidos\n",
    "        tweet['_id'] = tweet['id']\n",
    "        del tweet['id']\n",
    "        tweets_collections.insert_one(tweet)\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "    t_total = time.time() - t_inicial\n",
    "    if t_total > TEMPO_TOTAL_EM_MIN * 60:\n",
    "        break\n",
    "print(\"Tweets coletados: {}\".format(tweets_collections.count_documents({})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Item 2\n",
    "Com base no dataset gerado na questão 1, faça:\n",
    "  1. Separe todas as hashtags usadas nos Tweets.\n",
    "  2. Descubra a porcentagem de tweets com hashtags.\n",
    "  3. Identifique a hashtag mais popular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Todas hashtags: ['1MDoSport', '2022candidatocomecandocom13naoconfirmar', '2022gameoverlulismo', 'AM', 'AMExSPFC', 'ATENCIÓN', 'AlemDaIlusao', 'ApruebaSeraHermoso', 'Apruebo', 'AprueboSinMiedo', 'Arequipa', 'B22', 'BUNDAAWARDS', 'Bolsonaro22', 'BolsonaroNoPrimeiroTurno', 'BolsonaroReeleito', 'BritneyIsComing', 'BuenasNoches', 'BuenasNochesATodos', 'CHILE', 'CHVNoticias', 'CiroComAndreMarinho', 'CiroComAndréMarinho', 'CiroPresidente12', 'ConhecerParaPreservar', 'ConvivenciaEscolar', 'CopaChileEasy', 'CopaDoBrasil', 'CopadoMundo', 'CoquimboUnido', 'Cordoba', 'DailyPikminBloom', 'DaleRaya', 'Dentist', 'DesisteCiro', 'Disney', 'EdegarPrettoGovernador13', 'ForaBolsonaro', 'FueraAzulAzul', 'HoldMeCloser', 'Huachipato', 'Juevesdepiernas', 'KCAMexico', 'KOFXV', 'LATAM', 'LULADay', 'LeagueOfLegends', 'LulaNo1ºTurno', 'LulaPresidente', 'Marvel', 'MarvelStudios', 'MeuAniversárioNoTwitter', 'MiPaísImaginario', 'MiguelBustinduy', 'NFLPreseason', 'NemLulaNemBolsonaro', 'NewProfilePic', 'NÃORECUASÃOPAULO', 'OcapitaoVaiVencerOladrao', 'OlívioSenador131', 'PANTANAL', 'PEITOAWARDS', 'PINTOSAWARDS', 'Pantanal', 'Pantanar', 'Pase', 'PauloVieira', 'Plussizewoman', 'ReageVida', 'Recife', 'RodrigoGarciaPesadelo', 'RollingKitchenBRNoGNT', 'RollingKitchenBrasil', 'Ruta43', 'SNK', 'SP', 'SaludMental', 'ServilesVendenODIOxplata', 'SheHulk', 'SheHulkAttorneyAtLaw', 'SinFiltros', 'SteveRogers', 'TBT', 'TaconesRojos', 'TikTok', 'UTA', 'VamosLaU', 'Vamos_Azules', 'Vasco', 'Viernes', 'Yatra', 'YoApruebo', 'agendapolítica', 'amexsao', 'amexspfc', 'berrini', 'boanoite', 'brasildaesperança', 'cidadedagaroa', 'corno', 'cuckloud', 'darkside', 'debatecorreio', 'deputado', 'deusnocomando', 'espnequipof', 'fakenews', 'faleitoleve', 'globo', 'governador', 'hobipalloozabrasil2023', 'hotife', 'hotwifebrasil', 'humorbrasil', 'humorbrasileiro', 'humordodia', 'joaogomes', 'laneylatinoamerica', 'lorrainewarren', 'lula2022', 'lulaforadacadeiaégolpe', 'lulapresidente', 'lulapresidente13', 'lulavcsabequeperdeu', 'manaus', 'marcozero', 'memepage', 'memesengraçados', 'newells', 'odonto', 'pantanal', 'parabens', 'partiutrilhas', 'partiutrilhasetrip', 'pedradamacela', 'plussize', 'presidente', 'pss', 'quintafeira', 'rollingkitchenbrNoGNT', 'safadinha', 'saopaulo', 'saudadesdeumatrilha', 'sbt', 'sbtcompartilhe', 'sbtista', 'sbtnaweb', 'sbtonline', 'senador', 'spfc', 'stupidwife', 'tbt', 'tbtdeferias', 'tchutchucaDoCentrao', 'terracoitalia', 'thuthucadocentrao', 'travelling', 'trip', 'turistando', 'vemjhope', 'vidaseternas', 'wildrift']\n",
      "2. Tweets com hashtag: 4.82%\n",
      "3. Mais popular: tag=RodrigoGarciaPesadelo ocorrência=8\n"
     ]
    }
   ],
   "source": [
    "AGG_HASHTAGS_HISTOGRAM = [{'$project': {'entities.hashtags.text': 1}}, {'$match': {'entities.hashtags': {'$not': {'$size': 0}}}}, {'$project': {'hashtag': '$entities.hashtags.text'}}, {'$unwind': '$hashtag'}, {'$group': {'_id': '$hashtag', 'frequence': {'$sum': 1}}}, {'$sort': {'frequence': -1}}]\n",
    "AGG_HASHTAGS_TODAS = [{'$project': {'entities.hashtags.text': 1}}, {'$match': {'entities.hashtags': {'$not': {'$size': 0}}}}, {'$project': {'hashtag': '$entities.hashtags.text'}}, {'$unwind': '$hashtag'}, {'$group': {'_id': None, 'hashtags': {'$addToSet': '$hashtag'}}}, {'$unwind': '$hashtags'}, {'$sort': {'hashtags': 1}}, {'$group': {'_id': None, 'hashtags': {'$push': '$hashtags'}}}]\n",
    "AGG_HASHTAG_INFO = [{'$project': {'_id': '$_id', 'hashtag': {'$cond': [{'$gt': [{'$size': '$entities.hashtags'}, 0]}, 1, 0]}}}, {'$group': {'_id': None, 'total': {'$sum': 1}, 'total_hash': {'$sum': '$hashtag'}}}]\n",
    "\n",
    "## 1. Separe todas as hashtags usadas nos Tweets\n",
    "todas_hashtags = tweets_collections.aggregate(AGG_HASHTAGS_TODAS).next()\n",
    "print(\"1. Todas hashtags: {}\".format(todas_hashtags['hashtags']))\n",
    "\n",
    "info_hashtags = tweets_collections.aggregate(AGG_HASHTAG_INFO).next()\n",
    "print(\"2. Tweets com hashtag: {:0.2f}%\".format((info_hashtags['total_hash'] / info_hashtags['total']) * 100))\n",
    "\n",
    "popular_hashtag = tweets_collections.aggregate(AGG_HASHTAGS_HISTOGRAM).next()\n",
    "print(\"3. Mais popular: tag={} ocorrência={}\".format(popular_hashtag[\"_id\"], popular_hashtag[\"frequence\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Item 3\n",
    "\n",
    "Faça um coletor que capture as reviews da página principal de:\n",
    "https://www.apontador.com.br/local/sp/campinas/faculdades_e_universidades/88Q2E372/\n",
    "unicamp.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sucesso\n",
      "Nome:Renan Cunha De Sousa\n",
      "Commentário:  Unicamp é umas das mais cobiçadas universidades de Campinas e região, devido a sua imensa qualidade, por ser federal se torna a melhor faculdade da região. Sem contar o seu Campus que é sensacional, como se fosse uma cidade, possui quadras com prédios, até uma linha de ônibus circular que permanece dentro do campus apenas devido o seu tamanho. A universidade conta também com o Hospital das Clínicas, gratuito pelo SUS e administrar o hospital de Sumaré. As provas para ingressar na faculdade são por fases com respostas alternativas e requisitando cerca de 3 redações o número de candidatos por vaga é bem alto principalmente no curso de medicina, o problema fica no pagamento do vestibular que é cerca de 120~140 reais para poder executa-la. A graduação é em tempo integral com isso a faculdade possui centro de pesquisas, onde é possível fazer iniciações cientificas ou mini empresas, para que os alunos tenham uma breve praticidade no mercado de trabalho devido a dificuldade para o estagio devido ao tempo integral em aula. \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "req = requests.get(\"https://www.apontador.com.br/local/sp/campinas/faculdades_e_universidades/88Q2E372/unicamp.html\")\n",
    "if req.status_code >= 200 and req.status_code < 300:\n",
    "    print(\"Sucesso\")\n",
    "    soup = BeautifulSoup(req.content, 'html.parser')\n",
    "    for comentario in soup.find_all(name='div',attrs={'class':\"store-comments__container\"}):\n",
    "        print(\"\"\"Nome:        {}\n",
    "                 Commentário: {}\"\"\".format(comentario.find(attrs={'class':\"name\"}).get_text(),\n",
    "                                                comentario.find(attrs={'class':\"store-comments__text\"}).get_text()))\n",
    "else:\n",
    "    print(\"ERRO! {}\".format(req.status_code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import simplejson as json\n",
    "\n",
    "# Arquivo com Tweets\n",
    "tweets_file = open('saidaColetaStream.txt', \"r\")\n",
    "\n",
    "while True:\n",
    "  \n",
    "    #lê a linha do arquivo\n",
    "    tweet_json = tweets_file.readline()\n",
    "\n",
    "    if len(tweet_json)==0:\n",
    "        break\n",
    "\n",
    "    #remove espaços em branco\n",
    "    strippedJson = tweet_json.strip()\n",
    "\n",
    "    tweet = \"\"\n",
    "\n",
    "    try:\n",
    "        #converte uma string json em um objeto python\n",
    "        tweet = json.loads(strippedJson)\n",
    "    except:\n",
    "        continue \n",
    "\n",
    "    print(tweet['id']) # ID do tweet\n",
    "    print(tweet['created_at']) # data de postagem\n",
    "    print(tweet['text']) # texto do tweet\n",
    "\n",
    "    print(tweet['user']['id']) # id do usuário que postou\n",
    "    print(tweet['user']['name']) # nome do usuário\n",
    "    print(tweet['user']['screen_name']) # nome da conta do usuário \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web crawlers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exemplo 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "pagina = '<html><body><p>Bom dia, CSBC!</p></body></html>'\n",
    "soup = BeautifulSoup(pagina,'html.parser')\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exemplo 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "pagina = '<html><body><p>Bom dia, CSBC!</p><p>teste</p></body></html>'\n",
    "soup = BeautifulSoup(pagina,'html.parser')\n",
    "\n",
    "allP = soup.find_all('p')\n",
    "print(allP)\n",
    "\n",
    "print(allP[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exemplo 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "pagina = \"<table><tr><td>one</td><td>two</td></tr></table>\"\n",
    "soup = BeautifulSoup(pagina,'html.parser')\n",
    "\n",
    "allTR = soup.find_all('tr')\n",
    "print(allTR)\n",
    "\n",
    "allTD = soup.find_all('td')\n",
    "print(allTD)\n",
    "\n",
    "\n",
    "for t in soup.find_all('td'):\n",
    "    print(t)\n",
    "\n",
    "for t in soup.find_all('td'):\n",
    "    print(t.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exemplo - Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "url=\"https://pt.wikipedia.org/wiki/Universidade_Tecnol%C3%B3gica_Federal_do_Paran%C3%A1\"\n",
    "\n",
    "##retorna o conteudo da pagina\n",
    "req = requests.get(url)\n",
    "\n",
    "##transforma o conteudo da pagina em um objeto BeautifulSoup\n",
    "soup = BeautifulSoup(req.content,'html.parser')\n",
    "\n",
    "nomeBruto = soup.find(\"h1\",{\"id\":\"firstHeading\"})\n",
    "\n",
    "print(nomeBruto)\n",
    "\n",
    "print(nomeBruto.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as ec\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "##Informe o caminho do webdriver na sua máquina\n",
    "driver = webdriver.Chrome(executable_path='./chromedriver')\n",
    "\n",
    "\n",
    "driver.get(\"https://www.w3schools.com/xml/ajax_intro.asp\")  \n",
    "\n",
    "#Clica no botao da pagina para ver o conteudo     \n",
    "#o xpath do botao de interese eh: '//*[@id=\"demo\"]/button'\n",
    "botaolist = driver.find_elements_by_xpath('//*[@id=\"demo\"]/button')\n",
    "botaolist[0].click()\n",
    "\n",
    "wait = WebDriverWait(driver, 5)\n",
    "wait.until(ec.visibility_of_element_located((By.XPATH, '//*[@id=\"demo\"]/h1')))\n",
    "\n",
    "# Pega o fonte da página\n",
    "page_source = driver.page_source    \n",
    "\n",
    "#fecha o driver, mas quando estivermos coletando varias paginas podemos manter ativo pra nao precisar abrir o browser novamente\n",
    "#driver.close()\n",
    "    \n",
    "soup = BeautifulSoup(page_source, \"lxml\") #pega o conteúdo com beautifulsoup para parsing\n",
    "    \n",
    "content= soup.find(\"div\",{\"id\":\"demo\"}) \n",
    "print(content.text)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
